{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = [\n",
    "    \"purple is the best city in the forest\",\n",
    "    \"No way chimps go bananas for snacks!\",\n",
    "    \"it is not often you find soggy bananas on the street\",\n",
    "    \"green should have smelled more tranquil but somehow it just tasted rotten\",\n",
    "    \"joyce enjoyed eating pancakes with ketchup\",\n",
    "    \"throwing bananas on to the street is not art\",\n",
    "    \"as the asteroid hurtled toward earth becky was upset her dentist appointment had been canceled\",\n",
    "    \"I'm getting way too old. I don't even buy green bananas anymore.\",\n",
    "    \"to get your way you must not bombard the road with yellow fruit\",\n",
    "    \"Time flies like an arrow; fruit flies like a banana\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/vectordb/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "modules.json: 100%|██████████| 349/349 [00:00<00:00, 225kB/s]\n",
      "config_sentence_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 129kB/s]\n",
      "README.md: 100%|██████████| 9.85k/9.85k [00:00<00:00, 7.36MB/s]\n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 203kB/s]\n",
      "config.json: 100%|██████████| 591/591 [00:00<00:00, 2.71MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 438M/438M [00:44<00:00, 9.92MB/s] \n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/vectordb/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "tokenizer_config.json: 100%|██████████| 383/383 [00:00<00:00, 464kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.26MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 892kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 565kB/s]\n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 213kB/s]\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/vectordb/lib/python3.10/site-packages/transformers/models/mpnet/modeling_mpnet.py:1054: UserWarning: cumsum_out_mps supported by MPS on MacOS 13+, please upgrade (Triggered internally at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1699313532615/work/aten/src/ATen/native/mps/operations/UnaryOps.mm:406.)\n",
      "  incremental_indices = torch.cumsum(mask, dim=1).type_as(mask) * mask\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('flax-sentence-embeddings/all_datasets_v3_mpnet-base')\n",
    "\n",
    "# 10行のセンテンスを768次元のベクトルに変換（埋め込み）\n",
    "all_embeddings = model.encode(all_sentences)\n",
    "all_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01336706,  0.10373536, -0.00622339, ...,  0.03053999,\n",
       "         0.01826637, -0.0170403 ],\n",
       "       [ 0.0396864 , -0.00628764, -0.02050516, ...,  0.00605086,\n",
       "        -0.05281233, -0.0341568 ],\n",
       "       [ 0.05413862,  0.01059024, -0.01934125, ..., -0.0081793 ,\n",
       "        -0.02009751,  0.00830053],\n",
       "       ...,\n",
       "       [ 0.04934242,  0.06136308,  0.01472958, ...,  0.01373632,\n",
       "        -0.01050778, -0.00448923],\n",
       "       [ 0.06331057,  0.09323005,  0.00229201, ..., -0.0064486 ,\n",
       "         0.04311284, -0.02979367],\n",
       "       [ 0.04161606,  0.03788033, -0.01343606, ...,  0.00287843,\n",
       "         0.01492751, -0.03070068]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 118kB/s]\n",
      "config.json: 100%|██████████| 570/570 [00:00<00:00, 1.85MB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 705kB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 920kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['purple', 'is', 'the', 'best', 'city', 'in', 'the', 'forest']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 指定したtokenizerを用いる\n",
    "# 今回はbert-base-uncasedを使用\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "all_tokens = [tokenizer.tokenize(sentence.lower()) for sentence in all_sentences]\n",
    "all_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['purple', 'is', 'the', 'best', 'city', 'in', 'the', 'forest'],\n",
       " ['no', 'way', 'chi', '##mps', 'go', 'bananas', 'for', 'snacks', '!'],\n",
       " ['it',\n",
       "  'is',\n",
       "  'not',\n",
       "  'often',\n",
       "  'you',\n",
       "  'find',\n",
       "  'so',\n",
       "  '##ggy',\n",
       "  'bananas',\n",
       "  'on',\n",
       "  'the',\n",
       "  'street'],\n",
       " ['green',\n",
       "  'should',\n",
       "  'have',\n",
       "  'smelled',\n",
       "  'more',\n",
       "  'tran',\n",
       "  '##quil',\n",
       "  'but',\n",
       "  'somehow',\n",
       "  'it',\n",
       "  'just',\n",
       "  'tasted',\n",
       "  'rotten'],\n",
       " ['joyce', 'enjoyed', 'eating', 'pancakes', 'with', 'ke', '##tch', '##up'],\n",
       " ['throwing', 'bananas', 'on', 'to', 'the', 'street', 'is', 'not', 'art'],\n",
       " ['as',\n",
       "  'the',\n",
       "  'asteroid',\n",
       "  'hurt',\n",
       "  '##led',\n",
       "  'toward',\n",
       "  'earth',\n",
       "  'becky',\n",
       "  'was',\n",
       "  'upset',\n",
       "  'her',\n",
       "  'dentist',\n",
       "  'appointment',\n",
       "  'had',\n",
       "  'been',\n",
       "  'canceled'],\n",
       " ['i',\n",
       "  \"'\",\n",
       "  'm',\n",
       "  'getting',\n",
       "  'way',\n",
       "  'too',\n",
       "  'old',\n",
       "  '.',\n",
       "  'i',\n",
       "  'don',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'even',\n",
       "  'buy',\n",
       "  'green',\n",
       "  'bananas',\n",
       "  'anymore',\n",
       "  '.'],\n",
       " ['to',\n",
       "  'get',\n",
       "  'your',\n",
       "  'way',\n",
       "  'you',\n",
       "  'must',\n",
       "  'not',\n",
       "  'bomb',\n",
       "  '##ard',\n",
       "  'the',\n",
       "  'road',\n",
       "  'with',\n",
       "  'yellow',\n",
       "  'fruit'],\n",
       " ['time',\n",
       "  'flies',\n",
       "  'like',\n",
       "  'an',\n",
       "  'arrow',\n",
       "  ';',\n",
       "  'fruit',\n",
       "  'flies',\n",
       "  'like',\n",
       "  'a',\n",
       "  'banana']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pineconeの初期化\n",
    "from pinecone import Pinecone, PodSpec\n",
    "pc = Pinecone(api_key='1a0e3dd8-12dd-42cc-92ef-16e20a38cb4b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インデックスの作成\n",
    "# all_tokensを格納するDBの作成\n",
    "pc.create_index(\n",
    "    name=\"keyword-search-example\", \n",
    "    dimension=all_embeddings.shape[1], \n",
    "    metric=\"euclidean\",\n",
    "    spec=PodSpec(\n",
    "        environment='gcp-starter', \n",
    "        pod_type='starter'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インデックスに接続\n",
    "index = pc.Index(name=\"keyword-search-example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 10}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_embeddingsとall_tokensをInsertする\n",
    "\n",
    "# all_embeddings...単語のベクトル埋め込み（数値）\n",
    "# all_tokens...単語のベクトル\n",
    "upserts = []\n",
    "for i, (embedding, tokens) in enumerate(zip(all_embeddings, all_tokens)):\n",
    "    upserts.append((str(i), embedding.tolist(), {'tokens': tokens}))\n",
    "\n",
    "# 10行の文章が10個のベクトルとして格納される\n",
    "index.upsert(vectors=upserts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': '5',\n",
       "              'metadata': {'tokens': ['throwing',\n",
       "                                      'bananas',\n",
       "                                      'on',\n",
       "                                      'to',\n",
       "                                      'the',\n",
       "                                      'street',\n",
       "                                      'is',\n",
       "                                      'not',\n",
       "                                      'art']},\n",
       "              'score': 0.533911705,\n",
       "              'values': []},\n",
       "             {'id': '8',\n",
       "              'metadata': {'tokens': ['to',\n",
       "                                      'get',\n",
       "                                      'your',\n",
       "                                      'way',\n",
       "                                      'you',\n",
       "                                      'must',\n",
       "                                      'not',\n",
       "                                      'bomb',\n",
       "                                      '##ard',\n",
       "                                      'the',\n",
       "                                      'road',\n",
       "                                      'with',\n",
       "                                      'yellow',\n",
       "                                      'fruit']},\n",
       "              'score': 0.851906657,\n",
       "              'values': []},\n",
       "             {'id': '2',\n",
       "              'metadata': {'tokens': ['it',\n",
       "                                      'is',\n",
       "                                      'not',\n",
       "                                      'often',\n",
       "                                      'you',\n",
       "                                      'find',\n",
       "                                      'so',\n",
       "                                      '##ggy',\n",
       "                                      'bananas',\n",
       "                                      'on',\n",
       "                                      'the',\n",
       "                                      'street']},\n",
       "              'score': 0.996979713,\n",
       "              'values': []},\n",
       "             {'id': '1',\n",
       "              'metadata': {'tokens': ['no',\n",
       "                                      'way',\n",
       "                                      'chi',\n",
       "                                      '##mps',\n",
       "                                      'go',\n",
       "                                      'bananas',\n",
       "                                      'for',\n",
       "                                      'snacks',\n",
       "                                      '!']},\n",
       "              'score': 1.24630213,\n",
       "              'values': []},\n",
       "             {'id': '9',\n",
       "              'metadata': {'tokens': ['time',\n",
       "                                      'flies',\n",
       "                                      'like',\n",
       "                                      'an',\n",
       "                                      'arrow',\n",
       "                                      ';',\n",
       "                                      'fruit',\n",
       "                                      'flies',\n",
       "                                      'like',\n",
       "                                      'a',\n",
       "                                      'banana']},\n",
       "              'score': 1.32272363,\n",
       "              'values': []}],\n",
       " 'namespace': '',\n",
       " 'usage': {'read_units': 6}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# セマンティック検索\n",
    "query_sentence = \"there is an art to getting your way and throwing bananas on to the street is not it\"\n",
    "xq = model.encode([query_sentence]).tolist()\n",
    "\n",
    "result = index.query(vector=xq, top_k=5, includeMetadata=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': '5', 'score': 0.533911705, 'values': []},\n",
       "             {'id': '2', 'score': 0.996979713, 'values': []},\n",
       "             {'id': '1', 'score': 1.24630213, 'values': []},\n",
       "             {'id': '7', 'score': 1.35194838, 'values': []}],\n",
       " 'namespace': '',\n",
       " 'usage': {'read_units': 5}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ハイブリッド検索（セマンティック検索＋キーワード検索）\n",
    "# filterについて：https://docs.pinecone.io/docs/metadata-filtering\n",
    "result = index.query(vector=xq, top_k=10, filter={'tokens': 'bananas'})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "throwing bananas on to the street is not art\n",
      "it is not often you find soggy bananas on the street\n",
      "No way chimps go bananas for snacks!\n",
      "I'm getting way too old. I don't even buy green bananas anymore.\n"
     ]
    }
   ],
   "source": [
    "# 対象となるIDをもとに検索結果となるセンテンスを表示\n",
    "ids = [int(match['id']) for match in result['matches']]\n",
    "for i in ids:\n",
    "    print(all_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectordb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
